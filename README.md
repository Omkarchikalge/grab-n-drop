## Overview
This module provides real-time hand gesture recognition for the Grab-N-Drop platform.
It detects user intent through natural hand movements and emits discrete gesture events
that can be consumed by backend systems (WebSocket / WebRTC / P2P).

# ğŸ” What this module does
1. Uses MediaPipe Hand Landmarks for real-time hand tracking
2. Uses a temporal LSTM model to understand hand motion
3. Runs fully on-device (no video upload)
4. Prints gesture events to console output

# Gesture events:
GRAB
DROP

These events are intended to be consumed by:
WebSocket server
WebRTC data channel
Any event-based backend logic

# ğŸ§  Gesture Definitions (Important)
Gesture	Meaning	Hand Action
GRAB	Start / pick / hold	Close hand (fist)
DROP	Release / send	Open hand (palm)

# Gesture flow:
IDLE â†’ GRAB â†’ HOLDING â†’ DROP â†’ IDLE
A DROP will never occur without a prior GRAB.


# ğŸ“ Project Structure
grab-n-drop/
â”‚
â”œâ”€â”€ live_inference.py        # ğŸ”´ MAIN FILE (run this)
â”œâ”€â”€ record_dataset.py        # Dataset recording (used during training)
â”œâ”€â”€ train_lstm.py            # Model training script
â”œâ”€â”€ gesture_model.h5         # Trained LSTM model
â”œâ”€â”€ hand_landmarker.task     # MediaPipe hand model
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


# âš™ï¸ Setup Instructions

1ï¸âƒ£ Create virtual environment
python -m venv venv
venv\Scripts\activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

â–¶ï¸ How to run (MOST IMPORTANT)
Start gesture detection:
python live_inference.py


# Expected console output:

Live inference started â€” press ESC to exit
GRAB
DROP

# ğŸ–¨ï¸ Console Output Contract (VERY IMPORTANT)

The AI only prints:
GRAB
DROP
One event per line
Printed only when a gesture is detected
No continuous spam
No JSON
No extra logs (ignore TensorFlow warnings)

ğŸ‘‰ This output is the integration interface.

## Expected message mapping:(after integration)
{
  "type": "GESTURE",
  "action": "GRAB"
}


or

{
  "type": "GESTURE",
  "action": "DROP"
}

# Important rules:

1. Treat gestures like button presses

2. Do NOT expect continuous data

3. Do NOT request landmarks or video

4. Backend controls what GRAB/DROP means

## ğŸ§ª How to test gestures manually

Run python live_inference.py

Show hand to camera

Close hand â†’ GRAB

Hold fist â†’ no output

Open hand â†’ DROP

Repeat

If this works â†’ integration is safe.

## ğŸš« What this module does NOT do

âŒ No networking

âŒ No WebSocket code

âŒ No file transfer logic

âŒ No UI rendering logic

This separation is intentional and correct.

## ğŸ§  Design Philosophy

â€œAI detects intent, backend decides action.â€

This makes the system:

Modular

Scalable

Replaceable

Easy to debug

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Handoff Summary (Important)

Run live_inference.py

Read stdout

On GRAB â†’ start action

On DROP â†’ complete action

Ignore everything else

Thatâ€™s it.

## âœ… Status

Model: Stable

Latency: Real-time

Accuracy: Good for MVP / hackathon

Integration: Ready
